{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1_Owjbh-HpoUKqCK4plQmzf0pHah922RX","authorship_tag":"ABX9TyPL0ODlu99evxeTjb3/NgFf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CycleGAN : 이미지를 이미지로 변환하는 GAN 모델\n","* 이미지 --> 이미지 색조 등의 변환 --> 다시 이미지 재배치 해서 성능 비교\n","* 판별자 2개와 생성자 2개(A->B & B->A)"],"metadata":{"id":"XyiL5uTXUqwV"}},{"cell_type":"code","source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BV10OhplvQD2","executionInfo":{"status":"ok","timestamp":1662343727433,"user_tz":-540,"elapsed":5524,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"d8bdae44-25f0-48a0-af60-1eeeb95e0a25"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-dqucae5d\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-dqucae5d\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.8.0)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=3416430f743405b879663d122dba6c0e75c6cdd5e709a4ab5e20ae7a2e73419a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9b9ctzqx/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"]}]},{"cell_type":"code","source":["!pip install scipy==1.1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXhIV-Axjtp-","executionInfo":{"status":"ok","timestamp":1662343737635,"user_tz":-540,"elapsed":10206,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"0362d280-b2f2-4d23-f89a-49724dbe040a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.1.0\n","  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n","\u001b[K     |████████████████████████████████| 31.2 MB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.21.6)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pymc 4.1.4 requires scipy>=1.4.1, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.1.0 which is incompatible.\n","jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.1.0 which is incompatible.\n","jax 0.3.14 requires scipy>=1.5, but you have scipy 1.1.0 which is incompatible.\n","aeppl 0.0.33 requires scipy>=1.4.0, but you have scipy 1.1.0 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.1.0\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OtMC6KWQLaWz","executionInfo":{"status":"ok","timestamp":1662343798790,"user_tz":-540,"elapsed":1537,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}}},"outputs":[],"source":["from __future__ import print_function, division\n","from tensorflow.keras.datasets import mnist\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","from tensorflow.keras.layers import (Input, Dense, Reshape, Flatten, Dropout,\n","                                     Concatenate, BatchNormalization, Activation, ZeroPadding2D,\n","                                     LeakyReLU, UpSampling2D, Conv2D)\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from glob import glob\n","import scipy\n","import datetime\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sys\n","import os"]},{"cell_type":"code","source":["# DataLoader --> CycleGAN 학습에 필요한 데이터 로드\n","\n","class DataLoader():\n","\n","  def __init__(self, dataset_name, img_res=(128,128)):\n","    \n","    self.dataset_name = dataset_name\n","    self.img_res = img_res\n","  \n","\n","  def load_data(self, domain, batch_size=1, is_testing=False):\n","    \n","    data_type = 'train%s' % domain if not is_testing else 'test%s' % domain\n","    path = glob('/content/drive/MyDrive/com_vision_study/data/%s/%s/*' % (self.dataset_name, data_type))    # self.dataset_name(apple2orange) 및 data_type(A또는B) 지정\n","\n","    batch_images = np.random.choice(path, size=batch_size)  # batch_size 크기에 맞게 이미지 랜덤 추출\n","\n","    imgs = []\n","\n","    for img_path in batch_images:\n","      img = self.imread(img_path)\n","\n","      if not is_testing:  # is_testing=True 이면\n","        img = scipy.misc.imresize(img, self.img_res)   # 이미지 사이즈 조정\n","\n","        if np.random.random() > 0.5:\n","          img = np.fliplr(img)  # np.fliplr --> 이미지 상하좌우 전체반전\n","        \n","      else:   #is_testing=False이면\n","        img = scipy.misc.imresize(img, self.img_res)\n","      \n","      imgs.append(img)\n","    \n","    imgs = np.array(imgs) / 127.5 - 1\n","\n","    return imgs\n","\n","\n","  def load_batch(self, batch_size=1, is_testing=False):\n","\n","    data_type = 'train' if not is_testing else 'val'\n","    path_A = glob('/content/drive/MyDrive/com_vision_study/data/%s/%sA/*' %(self.dataset_name, data_type)) # train/test 중 data_type에 맞는 A 로드\n","    path_B = glob('/content/drive/MyDrive/com_vision_study/data/%s/%sB/*' %(self.dataset_name, data_type)) # train/test 중 data_type에 맞는 B 로드\n","\n","    self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n","    total_samples = self.n_batches * batch_size\n","\n","    path_A = np.random.choice(path_A, total_samples, replace=False)\n","    path_B = np.random.choice(path_B, total_samples, replace=False)\n","    \n","    for i in range(self.n_batches-1):\n","      batch_A = path_A[i*batch_size:(i+1)*batch_size]\n","      batch_B = path_B[i*batch_size:(i+1)*batch_size]\n","      imgs_A, imgs_B = [], []\n","\n","      for img_A, img_B in zip(batch_A, batch_B):\n","        img_A = self.imread(img_A)\n","        img_B = self.imread(img_B)\n","\n","        img_A = scipy.misc.imresize(img_A, self.img_res)\n","        img_B = scipy.misc.imresize(img_B, self.img_res)\n","\n","        if not is_testing and np.random.random() > 0.5:\n","          img_A = np.fliplr(img_A) \n","          img_B = np.fliplr(img_B)\n","        \n","        imgs_A.append(img_A)\n","        imgs_B.append(img_B)\n","    \n","      imgs_A = np.array(imgs_A) / 127.5-1\n","      imgs_B = np.array(imgs_B) / 127.5-1\n","\n","      yield imgs_A, imgs_B\n","  \n","  def imread(self, path):\n","    '''\n","    load_data에서 쓰일 imread하는 함수\n","    '''\n","    return scipy.misc.imread(path, mode='RGB').astype(np.float)"],"metadata":{"id":"P3Ir_Nu7umYm","executionInfo":{"status":"ok","timestamp":1662343803678,"user_tz":-540,"elapsed":520,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Cycle GAN\n","\n","class CycleGAN():\n","\n","  def __init__(self):\n","\n","    # input_image shape\n","    self.img_rows = 128\n","    self.img_cols = 128\n","    self.channels = 3\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","\n","    self.dataset_name = 'apple2orange'\n","    # DataLoader 클래스를 사용해 데이터 로드\n","    self.data_loader = DataLoader(dataset_name=self.dataset_name, \n","                                  img_res = (self.img_rows, self.img_cols))\n","    \n","    patch = int(self.img_rows / 2**4) # patchGAN의 크기(output shape of D)\n","    self.disc_patch = (patch, patch, 1)\n","\n","    # Generator와 Discriminator의 첫번째 층에 들어갈 필터 개수\n","    self.gf = 32\n","    self.df = 64\n","\n","    self.lambda_cycle = 10  # 사이클-일관선 손실 가중치\n","    self.lambda_id = 0.9 * self.lambda_cycle   # 동일성 손실 가중치\n","    optimizer = Adam(0.0002, 0.5)\n","\n","\n","    # 신경망 구성\n","\n","    # 1. 2개의 판별자 d_A, d_B 만든 후 compile\n","    self.d_A = self.build_discriminator()\n","    self.d_B = self.build_discriminator()\n","    self.d_A.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n","    self.d_B.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n","\n","    # 2. Generator A-->B, Generator B-->A 생성\n","    self.g_AB = self.build_generator()\n","    self.g_BA = self.build_generator()\n","\n","    # 3. 양방향 학습에 쓸 입력 이미지 구성\n","    img_A = Input(shape=self.img_shape)\n","    img_B = Input(shape=self.img_shape)\n","\n","    # 4. 각 이미지(img_A, img_B)를 각각 다른 도메인으로 변환\n","    fake_B = self.g_AB(img_A)\n","    fake_A = self.g_BA(img_B)\n","\n","    # 5. 원본 이미지로 이미지 재변환\n","    reconstr_A = self.g_BA(fake_B)\n","    reconstr_B = self.g_AB(fake_A)\n","\n","    # 동일한 이미지 매핑\n","    img_A_id = self.g_BA(img_A)\n","    img_B_id = self.g_AB(img_B)\n","\n","    # 연결 모델에서는 생성자만을 훈련\n","    self.d_A.trainable = False\n","    self.d_B.trainable = False\n","    \n","    # 판별자가 변환된 이미지의 유효성을 결정\n","    valid_A = self.d_A(fake_A)\n","    valid_B = self.d_B(fake_B)\n","\n","    # 연결모델 --> input으로 img_A와 img_B\n","    #        --> output으로 valid_A, valid_B, reconstr_A, reconstr_B, img_A_id, img_B_id\n","    self.combined = Model(inputs=[img_A, img_B],\n","                          outputs = [valid_A, valid_B,\n","                                     reconstr_A, reconstr_B,\n","                                     img_A_id, img_B_id])\n","    self.combined.compile(loss=['mse', 'mse',\n","                                'mae', 'mae',\n","                                'mae', 'mae'],\n","                          loss_weights=[1,1,\n","                                        self.lambda_cycle, self.lambda_cycle,\n","                                        self.lambda_id, self.lambda_id],\n","                          optimizer=optimizer)"],"metadata":{"id":"5KE0430q2vHc","executionInfo":{"status":"ok","timestamp":1662343803678,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 위에서 만든 CycleGAN()을 상속시켜 generator및 discriminator, sample_image생성, train을 진행할 것\n","\n","# 1. generator 및 discriminato에서 사용할 conv2d 및 deconv2d(=upsampling) 정의\n","\n","class CycleGAN(CycleGAN):\n","      \n","      def conv2d(layer_input, filters, f_size=4, normalization=True):\n","        '''\n","        conv2d for downsampling\n","        순서\n","        1) 일반적인 Conv2D 합성곱층\n","        2) LeakyReLU 활성화 함수\n","        3) InstanceNormalization(샘플 정규화 층)\n","        '''\n","        d = Conv2D(filters, kernel_size=f_size,\n","                   strides=2, padding='same')(layer_input)\n","        d = LeakyReLU(alpha=0.2)(d)\n","        if normalization:\n","            d = InstanceNormalization()(d)  # 샘플 정규화 층\n","        return d\n","      \n","      def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","        '''\n","        deconv2d for upsamling\n","        순서\n","        1) input_layer에 대해 Upsampling\n","        2) 드롭아웃 비율을 지정했다면 드롭아웃 적용\n","        3) InstanceNormalization\n","        4) 출력층과 출력층의 차원에 대응하는 downsampling부분에 있는 층의 skip_connection\n","        '''\n","\n","        u = UpSampling2D(size=2)(layer_input)\n","        u = Conv2D(filters, kernel_size=f_size, strides=1,\n","                    padding='same', activation='relu')(u)\n","        if dropout_rate:\n","            u = Dropout(dropout_rate)(u)\n","        u = InstanceNormalization()(u)\n","        u = Concatenate()([u, skip_input])\n","        return u"],"metadata":{"id":"wmhprO6UuR4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build Generator & Discriminator\n","\n","class CycleGAN(CycleGAN):\n","    def build_generator(self):\n","        '''\n","        U-Net구조의 Generator --> ResNet으로도 변환 가능\n","        '''\n","        # Image 입력\n","        d0 = Input(shape=self.img_shape) #(128,128,3)\n","\n","        # Downsampling\n","        d1 = self.conv2d(d0, self.gf)  # filters=32  --> (64,64,32)\n","        d2 = self.conv2d(d1, self.gf * 2)  # filters=64  --> (32,32,64)\n","        d3 = self.conv2d(d2, self.gf * 4)  # filters=128 --> (16,16,128)\n","        d4 = self.conv2d(d3, self.gf * 8)  # filters=256 --> (8,8,256)\n","\n","        # Upsampling\n","        u1 = self.deconv2d(d4, d3, self.gf * 4)   # layer_input=d4, skip_input=d3, filters=128\n","        u2 = self.deconv2d(u1, d2, self.gf * 2)   # layer_input=u1, skip_input=d2, filters=64 --> u1의 출력을 거치면 filters=64=d2\n","        u3 = self.deconv2d(u2, d1, self.gf)       # layer_input=d4, skip_input=d3, filters=32 --> u2의 출력을 거치면 filters=32=d1\n","\n","        u4 = UpSampling2D(size=2)(u3)   # (128,128,64)\n","        output_img = Conv2D(self.channels, kernel_size=4,\n","                            strides=1, padding='same', activation='tanh')(u4)\n","\n","        return Model(d0, output_img) \n","\n","class CycleGAN(CycleGAN):\n","    def build_discriminator(self):\n","      img = Input(shape=self.img_shape)  # (128, 128, 3)\n","\n","      d1 = self.conv2d(img, self.df, normalization=False)  # (64,64,64)\n","      d2 = self.conv2d(d1, self.df * 2)   # (32,32,128)\n","      d3 = self.conv2d(d2, self.df * 4)   # (16,16,256)\n","      d4 = self.conv2d(d3, self.df * 8)   # (8,8,512)\n","\n","      validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)   # (8,8,1)\n","\n","      return Model(img, validity)"],"metadata":{"id":"fUaJ0a8fonH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CycleGAN(CycleGAN):\n","  \n","      def sample_images(self, epoch, batch_i):\n","        r, c = 2, 3\n","\n","        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n","        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n","        \n","        # Translate images to the other domain\n","        fake_B = self.g_AB.predict(imgs_A)\n","        fake_A = self.g_BA.predict(imgs_B)\n","        # Translate back to original domain\n","        reconstr_A = self.g_BA.predict(fake_B)\n","        reconstr_B = self.g_AB.predict(fake_A)\n","\n","        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        titles = ['Original', 'Translated', 'Reconstructed']\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt])\n","                axs[i, j].set_title(titles[j])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(f\"{self.dataset_name}_{epoch}_{batch_i}\")\n","        plt.show()"],"metadata":{"id":"wo9dPhABilTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CycleGAN(CycleGAN):\n","  \n","      def train(self, epochs, batch_size=1, sample_interval=50):\n","        # Adversarial loss ground truths\n","        valid = np.ones((batch_size,) + self.disc_patch)\n","        fake = np.zeros((batch_size,) + self.disc_patch)\n","\n","\n","        for epoch in range(epochs):\n","            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n","\n","                # ----------------------\n","                #  Train Discriminators\n","                # ----------------------\n","\n","                # Translate images to opposite domain\n","                fake_B = self.g_AB.predict(imgs_A)\n","                fake_A = self.g_BA.predict(imgs_B)\n","\n","                # Train the discriminators (original images = real / translated = Fake)\n","                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n","                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n","                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n","                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n","                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","                # Total discriminator loss\n","                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","\n","                # ------------------\n","                #  Train Generators\n","                # ------------------\n","\n","                # Train the generators\n","                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n","                                                      [valid, valid,\n","                                                       imgs_A, imgs_B,\n","                                                       imgs_A, imgs_B])\n","                # If at save interval => plot the generated image samples\n","                if batch_i % sample_interval == 0:\n","                    self.sample_images(epoch, batch_i)"],"metadata":{"id":"ijrn0CK8iouZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cycle_gan = CycleGAN()\n","cycle_gan.train(epochs=100, batch_size=64, sample_interval=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dWcmmbFnuXk_4J2Q1XI0LzY_xdmCLQqi"},"id":"d9agWr2_iueB","outputId":"622e3a1e-dae1-4762-c74a-43af54a4f956","executionInfo":{"status":"ok","timestamp":1662348781250,"user_tz":-540,"elapsed":4968361,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}}},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["class CycleGAN(CycleGAN):\n","      @staticmethod\n","      def conv2d(layer_input, filters, f_size=4, normalization=True):\n","        \"\"\"Discriminator layer\"\"\"\n","        d = Conv2D(filters, kernel_size=f_size,\n","                   strides=2, padding='same')(layer_input)\n","        d = LeakyReLU(alpha=0.2)(d)\n","        if normalization:\n","            d = InstanceNormalization()(d)\n","        return d\n","      \n","      @staticmethod\n","      def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","            \"\"\"Layers used during upsampling\"\"\"\n","            u = UpSampling2D(size=2)(layer_input)\n","            u = Conv2D(filters, kernel_size=f_size, strides=1,\n","                       padding='same', activation='relu')(u)\n","            if dropout_rate:\n","                u = Dropout(dropout_rate)(u)\n","            u = InstanceNormalization()(u)\n","            u = Concatenate()([u, skip_input])\n","            return u\n","\n","class CycleGAN(CycleGAN):\n","    def build_generator(self):\n","        \"\"\"U-Net Generator\"\"\"\n","        # Image input\n","        d0 = Input(shape=self.img_shape)\n","\n","        # Downsampling\n","        d1 = self.conv2d(d0, self.gf)\n","        d2 = self.conv2d(d1, self.gf * 2)\n","        d3 = self.conv2d(d2, self.gf * 4)\n","        d4 = self.conv2d(d3, self.gf * 8)\n","\n","        # Upsampling\n","        u1 = self.deconv2d(d4, d3, self.gf * 4)\n","        u2 = self.deconv2d(u1, d2, self.gf * 2)\n","        u3 = self.deconv2d(u2, d1, self.gf)\n","\n","        u4 = UpSampling2D(size=2)(u3)\n","        output_img = Conv2D(self.channels, kernel_size=4,\n","                            strides=1, padding='same', activation='tanh')(u4)\n","\n","        return Model(d0, output_img)\n","\n","class CycleGAN(CycleGAN):\n","    def build_discriminator(self):\n","      img = Input(shape=self.img_shape)\n","\n","      d1 = self.conv2d(img, self.df, normalization=False)\n","      d2 = self.conv2d(d1, self.df * 2)\n","      d3 = self.conv2d(d2, self.df * 4)\n","      d4 = self.conv2d(d3, self.df * 8)\n","\n","      validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n","\n","      return Model(img, validity)\n","\n","\n","class CycleGAN(CycleGAN):\n","      def sample_images(self, epoch, batch_i):\n","        r, c = 2, 3\n","\n","        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n","        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n","        \n","        # Translate images to the other domain\n","        fake_B = self.g_AB.predict(imgs_A)\n","        fake_A = self.g_BA.predict(imgs_B)\n","        # Translate back to original domain\n","        reconstr_A = self.g_BA.predict(fake_B)\n","        reconstr_B = self.g_AB.predict(fake_A)\n","\n","        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        titles = ['Original', 'Translated', 'Reconstructed']\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt])\n","                axs[i, j].set_title(titles[j])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(f\"{self.dataset_name}_{epoch}_{batch_i}\")\n","        plt.show()\n","\n","class CycleGAN(CycleGAN):\n","      def train(self, epochs, batch_size=1, sample_interval=50):\n","        # Adversarial loss ground truths\n","        valid = np.ones((batch_size,) + self.disc_patch)\n","        fake = np.zeros((batch_size,) + self.disc_patch)\n","\n","\n","        for epoch in range(epochs):\n","            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n","\n","                # ----------------------\n","                #  Train Discriminators\n","                # ----------------------\n","\n","                # Translate images to opposite domain\n","                fake_B = self.g_AB.predict(imgs_A)\n","                fake_A = self.g_BA.predict(imgs_B)\n","\n","                # Train the discriminators (original images = real / translated = Fake)\n","                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n","                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n","                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n","                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n","                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","                # Total discriminator loss\n","                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","\n","                # ------------------\n","                #  Train Generators\n","                # ------------------\n","\n","                # Train the generators\n","                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n","                                                      [valid, valid,\n","                                                       imgs_A, imgs_B,\n","                                                       imgs_A, imgs_B])\n","                # If at save interval => plot the generated image samples\n","                if batch_i % sample_interval == 0:\n","                    self.sample_images(epoch, batch_i)"],"metadata":{"id":"HluVaSkCwDzw","executionInfo":{"status":"ok","timestamp":1662343807943,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}}},"execution_count":8,"outputs":[]}]}