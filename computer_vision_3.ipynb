{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"computer_vision_3.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"17FbJ_x3-MdklGG1Qs0XZKxEPXJCn4kw_","authorship_tag":"ABX9TyP7edg/Zx4fsPo2Bl8WWTQc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 전이학습 : 사전훈련된 신경망 모델을 가져와 해결하려는 task의 시작점으로 삼는 것\n","* 방대한 이미지 데이터로 사전 학습된 신경망 모델의 가중치 및 특성 맵을 가져와 문제에 적용한다."],"metadata":{"id":"rONmK3uOFACB"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"WLWfoQosE1wU","executionInfo":{"status":"ok","timestamp":1660901596049,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}}},"outputs":[],"source":["# 1. Pre-trained VGG-16\n","from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from keras.preprocessing import image\n","from keras.applications import imagenet_utils\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input, decode_predictions\n","from keras.applications import mobilenet\n","from tensorflow.keras.optimizers import Adam, SGD\n","from keras.metrics import categorical_crossentropy\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import preprocessing\n","from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n","from keras.models import Model\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.datasets import load_files\n","from keras.utils import np_utils\n","from tqdm import tqdm\n","import numpy as np\n","import itertools\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# 전이학습 구현\n","# 1) 가중치를 포함한 가져올 모델의 오픈소스 구현을 내려받아 base_model을 만든다\n","#    include_top=False --> 분류기 부분의 가중치는 내려받지 않는다 --> 직접 우리가 분류기 및 가중치 설정\n","\n","# 2) model.summary()로 사전 훈련된 모델의 구조를 확인한다.\n","\n","# 3) 사전 훈련된 층의 가중치는 고정시킨다(freeze)\n","\n","# 4) 분류기 부분을 새로 구현해서 추가한다\n","\n","\n","\n","# 사전 훈련된 VGG-16모델을 활용한 분류기 구현\n","\n","# 1)\n","base_model = VGG16(weights='imagenet', include_top=False,\n","                   input_shape=(224,224,3)) # base_model에 imagenet데이터셋으로 학습된 가중치를 내려받음\n","                                            # include_top=False --> 분류기 부분의 가중치는 받지 않겠다!\n","                                            # input_shape : 우리 데이터셋의 shape 지정\n","# 2)                                          \n","print(base_model.summary())                                  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cz696HzmGqAT","executionInfo":{"status":"ok","timestamp":1660900506901,"user_tz":-540,"elapsed":4834,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"63ecec9c-07c8-4ac3-8b7c-8e0840c68e3a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","58900480/58889256 [==============================] - 1s 0us/step\n","Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# 3) \n","for layer in base_model.layers:\n","  layer.trainable = False\n","print(base_model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPgRVw8PIIUY","executionInfo":{"status":"ok","timestamp":1660900507801,"user_tz":-540,"elapsed":903,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"c28a750f-f1ab-4951-9b86-0444a2d4617f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# 4) \n","last_layer = base_model.get_layer('block5_pool')   # 신경망의 마지막 층(block5_pool)에 접근\n","last_output = last_layer.output   # 마지막 층의 출력을 변수로 가져옴\n","\n","x = Flatten()(last_output)\n","x = Dense(2, activation='softmax', name='softmax')(x)  # class 2개를 분류하는, softmax 함수를 활성화함수로 갖는 층 추가\n","\n","new_model = Model(inputs=base_model.input, outputs=x)\n","print(new_model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqfWwNtFIAHL","executionInfo":{"status":"ok","timestamp":1660900507802,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"3911b92c-2c7f-4ba8-a397-303ed4ecfb3e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," softmax (Dense)             (None, 2)                 50178     \n","                                                                 \n","=================================================================\n","Total params: 14,764,866\n","Trainable params: 50,178\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["# 전이학습의 3가지 방식\n","## 1. 사전학습된 신경망을 분류기로 이용하기\n","## 2. 사전학습된 신경망을 특징 추출기로만 이용하기\n","## 3. 미세조정(fine-tuning)하기"],"metadata":{"id":"ZtbsBe2bWei5"}},{"cell_type":"code","source":["# 1. 사전학습된 신경망을 분류기로 이용하기\n","# --> 사전 학습된 신경망의 가중치 freeze 및 추가학습이 필요하지 않음\n","# --> 비슷한 과업에 대해 사전학습된 신경망을 골라 직접 새로운 과업에 바로 투입(분류기 포함)\n","# --> 사전 학습 신경망에 쓰인 데이터셋과 새로운 데이터셋의 도메인이 매우 유사한 경우 사용\n","\n","model_2 = VGG16(weights='imagenet', include_top=True, input_shape=(224,224,3)) # include_top=True\n","\n","# 개(German_shepherd)를 사전학습된 VGG16 모델이 제대로 분류하는지 체크\n","image = load_img('/content/drive/MyDrive/com_vision_study/data/dog.jpg', target_size=(224,224))\n","image = img_to_array(image) # img_to_array : 이미지의 픽셀값을 numpy array 형태로 변환\n","print(image.shape)\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","print(type(image))\n","\n","image = preprocess_input(image)   # vgg16의 모듈 --> VGG모델 입력을 위해 numpy array데이터를 전처리\n","\n","yhat = model_2.predict(image)\n","label = decode_predictions(yhat) # predict로 부터 예측 클래스 추출\n","label = label[0][0]  # 가장 확률이 높은 클래스값 추출\n","\n","print(f'{label[1]} :  {label[2]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAZ9jx3VWkHz","executionInfo":{"status":"ok","timestamp":1660900529021,"user_tz":-540,"elapsed":21222,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"05f26cc9-dad1-4486-b59b-eefd34c08ecf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467904/553467096 [==============================] - 6s 0us/step\n","553476096/553467096 [==============================] - 6s 0us/step\n","(224, 224, 3)\n","<class 'numpy.ndarray'>\n","Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n","40960/35363 [==================================] - 0s 0us/step\n","49152/35363 [=========================================] - 0s 0us/step\n","German_shepherd :  0.9971912503242493\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"CSQGUcZEr8EC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. 사전학습된 신경망을 특징 추출기로만 이용하기\n","# 실습 - VGG16을 활용해 개/고양이 분류\n","\n","\n","# 이미지를 가져와 데이터 전처리\n","train_path = '/content/drive/MyDrive/com_vision_study/data/dogs_vs_cats_project/data/train'\n","valid_path = '/content/drive/MyDrive/com_vision_study/data/dogs_vs_cats_project/data/valid'\n","test_path = '/content/drive/MyDrive/com_vision_study/data/dogs_vs_cats_project/data/test'\n","\n","idg = ImageDataGenerator()\n","\n","train_batches = idg.flow_from_directory(train_path, target_size=(224,224), batch_size=10)\n","valid_batches = idg.flow_from_directory(valid_path, target_size=(224,224), batch_size=10)\n","test_batches = idg.flow_from_directory(test_path, target_size=(224,224), batch_size=10, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7h0VHNZS0N9M","executionInfo":{"status":"ok","timestamp":1660900534077,"user_tz":-540,"elapsed":5059,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"acb9e1cf-955d-4779-d707-5f96331f8628"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 202 images belonging to 2 classes.\n","Found 103 images belonging to 2 classes.\n","Found 451 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["kargs = {'dense_1' : 64, 'dropout_rate' : 0.5, 'dense_2' : 2}\n","\n","def pretrained_vgg_16_cat_dog(**kargs):\n","  base_model = VGG16(weights='imagenet', include_top=False,\n","                     input_shape=(224,224,3))\n","  \n","  for layer in base_model.layers:\n","    layer.trainable = False    # 사전학습 층 고정\n","  \n","  last_layer = base_model.get_layer('block5_pool')\n","  last_output = last_layer.output\n","\n","  x = Flatten()(last_output)  # last_output을 Dense layer에서 쓰이게끔 Flatten()\n","  x = Dense(kargs['dense_1'], activation='relu', name='FC_2')(x)\n","  x = BatchNormalization()(x)\n","  x = Dropout(kargs['dropout_rate'])(x)\n","  x = Dense(kargs['dense_2'], activation='softmax', name='softmax')(x)\n","\n","  new_model = Model(inputs=base_model.input, outputs=x)\n","  print(new_model.summary())\n","\n","  return new_model  \n","\n","model_3 = pretrained_vgg_16_cat_dog(**kargs)\n","model_3.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc'])\n","model_3.fit_generator(train_batches, steps_per_epoch=4, validation_data=valid_batches, \n","                      validation_steps=2, epochs=20, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RAR02U_YjNg","executionInfo":{"status":"ok","timestamp":1660901177716,"user_tz":-540,"elapsed":321398,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"1e1d1d5c-3d07-40de-cc8b-2a6c0ab36642"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 25088)             0         \n","                                                                 \n"," FC_2 (Dense)                (None, 64)                1605696   \n","                                                                 \n"," batch_normalization (BatchN  (None, 64)               256       \n"," ormalization)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," softmax (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 16,320,770\n","Trainable params: 1,605,954\n","Non-trainable params: 14,714,816\n","_________________________________________________________________\n","None\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","4/4 - 52s - loss: 1.4145 - acc: 0.5000 - val_loss: 2.2202 - val_acc: 0.6000 - 52s/epoch - 13s/step\n","Epoch 2/20\n","4/4 - 43s - loss: 0.6692 - acc: 0.6750 - val_loss: 2.3883 - val_acc: 0.6500 - 43s/epoch - 11s/step\n","Epoch 3/20\n","4/4 - 27s - loss: 0.6179 - acc: 0.7750 - val_loss: 1.0251 - val_acc: 0.8000 - 27s/epoch - 7s/step\n","Epoch 4/20\n","4/4 - 25s - loss: 0.3398 - acc: 0.8500 - val_loss: 1.1068 - val_acc: 0.7500 - 25s/epoch - 6s/step\n","Epoch 5/20\n","4/4 - 16s - loss: 0.6212 - acc: 0.7500 - val_loss: 1.3291 - val_acc: 0.8000 - 16s/epoch - 4s/step\n","Epoch 6/20\n","4/4 - 10s - loss: 0.2466 - acc: 0.8500 - val_loss: 0.4516 - val_acc: 0.8500 - 10s/epoch - 3s/step\n","Epoch 7/20\n","4/4 - 11s - loss: 0.1773 - acc: 0.9500 - val_loss: 0.4897 - val_acc: 0.8500 - 11s/epoch - 3s/step\n","Epoch 8/20\n","4/4 - 5s - loss: 0.3404 - acc: 0.8750 - val_loss: 0.4112 - val_acc: 0.8000 - 5s/epoch - 1s/step\n","Epoch 9/20\n","4/4 - 7s - loss: 0.2564 - acc: 0.8750 - val_loss: 0.3418 - val_acc: 0.8000 - 7s/epoch - 2s/step\n","Epoch 10/20\n","4/4 - 4s - loss: 0.1713 - acc: 0.9000 - val_loss: 0.7293 - val_acc: 0.8000 - 4s/epoch - 887ms/step\n","Epoch 11/20\n","4/4 - 5s - loss: 0.0881 - acc: 0.9500 - val_loss: 0.2393 - val_acc: 0.9000 - 5s/epoch - 1s/step\n","Epoch 12/20\n","4/4 - 3s - loss: 0.1386 - acc: 0.9062 - val_loss: 0.3107 - val_acc: 0.9000 - 3s/epoch - 761ms/step\n","Epoch 13/20\n","4/4 - 3s - loss: 0.1646 - acc: 0.9000 - val_loss: 0.5182 - val_acc: 0.8500 - 3s/epoch - 784ms/step\n","Epoch 14/20\n","4/4 - 2s - loss: 0.2320 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.8000 - 2s/epoch - 549ms/step\n","Epoch 15/20\n","4/4 - 1s - loss: 0.1265 - acc: 0.9688 - val_loss: 0.3948 - val_acc: 0.9000 - 1s/epoch - 308ms/step\n","Epoch 16/20\n","4/4 - 2s - loss: 0.1243 - acc: 0.9500 - val_loss: 0.3247 - val_acc: 0.9000 - 2s/epoch - 565ms/step\n","Epoch 17/20\n","4/4 - 3s - loss: 0.2398 - acc: 0.9000 - val_loss: 0.5262 - val_acc: 0.8500 - 3s/epoch - 635ms/step\n","Epoch 18/20\n","4/4 - 1s - loss: 0.0677 - acc: 0.9750 - val_loss: 0.3442 - val_acc: 0.9000 - 547ms/epoch - 137ms/step\n","Epoch 19/20\n","4/4 - 0s - loss: 0.1434 - acc: 0.9500 - val_loss: 0.2735 - val_acc: 0.9000 - 457ms/epoch - 114ms/step\n","Epoch 20/20\n","4/4 - 0s - loss: 0.1042 - acc: 1.0000 - val_loss: 0.1365 - val_acc: 0.9000 - 471ms/epoch - 118ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f8f66774790>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# 데이터셋을 가져와 이미지, 정답레이블로 분류하는 함수\n","def load_dataset(path):\n","  data = load_files(path)\n","  paths = np.array(data['filenames']) # 이미지 path\n","  targets = np_utils.to_categorical(np.array(data['target'])) # 정답값\n","  return paths, targets\n","\n","# test_files(테스트 이미지 데이터)를 텐서 형태로 변환하는 함수\n","def path_to_tensor(img_path):\n","  img = preprocessing.image.load_img(img_path, target_size=(224,224))  # PIL.Image.Image타입으로 데이터 로드  (224,224,3)\n","  x = image.img_to_array(img)  # (224,224,3)\n","  return np.expand_dims(x, axis=0) # (1, 224,224,3)\n","\n","def paths_to_tensor(img_paths):\n","  list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n","  return np.vstack(list_of_tensors)\n","\n","test_files, test_targets = load_dataset('/content/drive/MyDrive/com_vision_study/data/dogs_vs_cats_project/data/test')\n","test_tensors = preprocess_input(paths_to_tensor(test_files))\n","\n","\n","# test\n","model_3.evaluate(test_tensors, test_targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_x-woHJ7RWD","executionInfo":{"status":"ok","timestamp":1660901613706,"user_tz":-540,"elapsed":9075,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"8342cab4-8f27-41d5-ee7b-b844d03ded9c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","\n","  0%|          | 0/451 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  7%|▋         | 30/451 [00:00<00:01, 293.24it/s]\u001b[A\u001b[A\n","\n"," 13%|█▎        | 60/451 [00:00<00:01, 274.88it/s]\u001b[A\u001b[A\n","\n"," 20%|█▉        | 88/451 [00:00<00:01, 240.29it/s]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 113/451 [00:00<00:01, 177.03it/s]\u001b[A\u001b[A\n","\n"," 29%|██▉       | 133/451 [00:00<00:01, 171.79it/s]\u001b[A\u001b[A\n","\n"," 35%|███▍      | 156/451 [00:00<00:01, 185.58it/s]\u001b[A\u001b[A\n","\n"," 39%|███▉      | 177/451 [00:00<00:01, 191.25it/s]\u001b[A\u001b[A\n","\n"," 45%|████▌     | 204/451 [00:00<00:01, 209.77it/s]\u001b[A\u001b[A\n","\n"," 51%|█████     | 228/451 [00:01<00:01, 217.76it/s]\u001b[A\u001b[A\n","\n"," 56%|█████▌    | 252/451 [00:01<00:00, 222.72it/s]\u001b[A\u001b[A\n","\n"," 61%|██████    | 276/451 [00:01<00:00, 226.42it/s]\u001b[A\u001b[A\n","\n"," 66%|██████▋   | 299/451 [00:01<00:00, 224.14it/s]\u001b[A\u001b[A\n","\n"," 72%|███████▏  | 323/451 [00:01<00:00, 227.61it/s]\u001b[A\u001b[A\n","\n"," 77%|███████▋  | 346/451 [00:01<00:00, 227.05it/s]\u001b[A\u001b[A\n","\n"," 82%|████████▏ | 369/451 [00:01<00:00, 227.48it/s]\u001b[A\u001b[A\n","\n"," 87%|████████▋ | 392/451 [00:01<00:00, 225.59it/s]\u001b[A\u001b[A\n","\n"," 92%|█████████▏| 415/451 [00:01<00:00, 225.28it/s]\u001b[A\u001b[A\n","\n","100%|██████████| 451/451 [00:02<00:00, 218.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["15/15 [==============================] - 6s 192ms/step - loss: 0.1367 - acc: 0.9468\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.13667601346969604, 0.9467849135398865]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[""],"metadata":{"id":"O5-r9bve4CVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. 미세조정(fine-tuning)하기\n","\n","# 실습2 - VGG16 & 수화데이터셋(정답10개 : 0~9)\n","# VGG16의 사전 훈련된 데이터셋과 수화데이터셋(task로 주어진 데이터셋)이 다르므로 fine_tuning 필요\n","\n","train_path = '/content/drive/MyDrive/com_vision_study/data/sign_language_project/dataset/train'\n","valid_path = '/content/drive/MyDrive/com_vision_study/data/sign_language_project/dataset/valid'\n","test_path = '/content/drive/MyDrive/com_vision_study/data/sign_language_project/dataset/test'\n","\n","idg_2 = ImageDataGenerator()\n","\n","train_batches = idg_2.flow_from_directory(train_path, target_size=(224,224), batch_size=10)\n","valid_batches = idg_2.flow_from_directory(valid_path, target_size=(224,224), batch_size=30)\n","test_batches = idg_2.flow_from_directory(test_path, target_size=(224,224), batch_size=50, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqBcm1qM4CW6","executionInfo":{"status":"ok","timestamp":1660901647871,"user_tz":-540,"elapsed":7977,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"b49e7c0c-584f-4c3d-a7bf-42ccab214d57"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1712 images belonging to 10 classes.\n","Found 300 images belonging to 10 classes.\n","Found 50 images belonging to 10 classes.\n"]}]},{"cell_type":"code","source":["kargs_2 = {'dense' : 10, 'layer_name' : 'softmax'}\n","\n","def fine_tuned_vgg_16(**kargs_2):\n","  base_model = VGG16(weights='imagenet', include_top=False,\n","                     input_shape=(224,224,3), pooling='avg')\n","  # pooling='avg' --> 마지막 합성곱층의 출력에 AveragePooling을 적용하라는 뜻\n","\n","  for layer in base_model.layers[:-5]:     # layer[:-5] 전까지만 freeze 나머지는 분류기와 함께 fine_tuning\n","    layer.trainable=False\n","  \n","  last_output = base_model.output\n","  x = Dense(kargs_2['dense'], activation='softmax', name=kargs_2['layer_name'])(last_output) # 분류기\n","  new_model = Model(inputs=base_model.input, outputs=x)\n","\n","  print(new_model.summary())\n","  return new_model\n","\n","model_4 = fine_tuned_vgg_16(**kargs_2)\n","model_4.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc'])\n","es = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=4, verbose=0, restore_best_weights=True)\n","cp = ModelCheckpoint('pretrained_vgg16_handsign.h5', monitor='val_acc', save_best_only=True, save_weights_only=True)\n","\n","model_4.fit_generator(train_batches, steps_per_epoch=18, validation_data=valid_batches,\n","                      validation_steps=3, epochs=20, verbose=1, callbacks=[es,cp])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhFaLhn7c9YC","executionInfo":{"status":"ok","timestamp":1660903473454,"user_tz":-540,"elapsed":1528547,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"bbd031de-1cf4-4a4b-9745-7315daf6c5cc"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," softmax (Dense)             (None, 10)                5130      \n","                                                                 \n","=================================================================\n","Total params: 14,719,818\n","Trainable params: 7,084,554\n","Non-trainable params: 7,635,264\n","_________________________________________________________________\n","None\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","18/18 [==============================] - 262s 15s/step - loss: 3.4794 - acc: 0.1722 - val_loss: 2.2583 - val_acc: 0.1667\n","Epoch 2/20\n","18/18 [==============================] - 197s 11s/step - loss: 1.9932 - acc: 0.3111 - val_loss: 1.6333 - val_acc: 0.5000\n","Epoch 3/20\n","18/18 [==============================] - 160s 9s/step - loss: 1.3187 - acc: 0.5222 - val_loss: 0.9668 - val_acc: 0.6556\n","Epoch 4/20\n","18/18 [==============================] - 129s 7s/step - loss: 0.6185 - acc: 0.8167 - val_loss: 0.4636 - val_acc: 0.8444\n","Epoch 5/20\n","18/18 [==============================] - 101s 6s/step - loss: 0.3736 - acc: 0.9012 - val_loss: 0.2990 - val_acc: 0.8889\n","Epoch 6/20\n","18/18 [==============================] - 95s 5s/step - loss: 0.2698 - acc: 0.9167 - val_loss: 0.3548 - val_acc: 0.9444\n","Epoch 7/20\n","18/18 [==============================] - 82s 4s/step - loss: 0.1623 - acc: 0.9444 - val_loss: 0.1122 - val_acc: 0.9556\n","Epoch 8/20\n","18/18 [==============================] - 68s 4s/step - loss: 0.2399 - acc: 0.9000 - val_loss: 0.1439 - val_acc: 0.9444\n","Epoch 9/20\n","18/18 [==============================] - 75s 4s/step - loss: 0.1994 - acc: 0.9500 - val_loss: 0.3288 - val_acc: 0.9222\n","Epoch 10/20\n","18/18 [==============================] - 56s 3s/step - loss: 0.1585 - acc: 0.9389 - val_loss: 0.0982 - val_acc: 0.9778\n","Epoch 11/20\n","18/18 [==============================] - 46s 3s/step - loss: 0.1152 - acc: 0.9419 - val_loss: 0.3690 - val_acc: 0.8778\n","Epoch 12/20\n","18/18 [==============================] - 43s 2s/step - loss: 0.1506 - acc: 0.9500 - val_loss: 0.0734 - val_acc: 0.9889\n","Epoch 13/20\n","18/18 [==============================] - 40s 2s/step - loss: 0.1476 - acc: 0.9833 - val_loss: 0.1345 - val_acc: 0.9556\n","Epoch 14/20\n","18/18 [==============================] - 33s 2s/step - loss: 0.0292 - acc: 0.9889 - val_loss: 0.1082 - val_acc: 0.9556\n","Epoch 15/20\n","18/18 [==============================] - 25s 1s/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.2249 - val_acc: 0.9778\n","Epoch 16/20\n","18/18 [==============================] - 29s 2s/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2097 - val_acc: 0.9889\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f900ea28090>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["test_files, test_targets = load_dataset('/content/drive/MyDrive/com_vision_study/data/sign_language_project/dataset/test')\n","test_tensors = preprocess_input(paths_to_tensor(test_files))\n","\n","\n","model_4.load_weights('pretrained_vgg16_handsign.h5')\n","model_4.evaluate(test_tensors, test_targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oABB20Bfbjnp","executionInfo":{"status":"ok","timestamp":1660903528801,"user_tz":-540,"elapsed":55350,"user":{"displayName":"‍류동엽[학생](대학원 빅데이터응용학과)","userId":"06943825373448622702"}},"outputId":"6ab24d85-4836-47fb-c71d-a59c5992258a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n","100%|██████████| 50/50 [00:00<00:00, 318.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 2s 2s/step - loss: 0.3024 - acc: 0.9400\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3023955821990967, 0.9399999976158142]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[""],"metadata":{"id":"Dh-eIx7heovi"},"execution_count":null,"outputs":[]}]}